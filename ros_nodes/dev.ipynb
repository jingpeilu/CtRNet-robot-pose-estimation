{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from ros_nodes.particle_filter import *\n",
    "from ros_nodes.probability_funcs import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from models.CtRNet import CtRNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.base_dir = \"/home/workspace/src/ctrnet-robot-pose-estimation-ros\"\n",
    "args.use_gpu = True\n",
    "args.trained_on_multi_gpus = True\n",
    "args.keypoint_seg_model_path = os.path.join(args.base_dir,\"weights/panda/panda-3cam_azure/net.pth\")\n",
    "args.urdf_file = os.path.join(args.base_dir,\"urdfs/Panda/panda.urdf\")\n",
    "\n",
    "args.robot_name = 'Panda' # \"Panda\" or \"Baxter_left_arm\"\n",
    "args.n_kp = 7\n",
    "args.height = 480\n",
    "args.width = 640\n",
    "args.fx, args.fy, args.px, args.py = 399.6578776041667, 399.4959309895833, 319.8955891927083, 244.0602823893229\n",
    "args.scale = 0.5 # scale the input image size to (320,240)\n",
    "\n",
    "# scale the camera parameters\n",
    "args.width = int(args.width * args.scale)\n",
    "args.height = int(args.height * args.scale)\n",
    "args.fx = args.fx * args.scale\n",
    "args.fy = args.fy * args.scale\n",
    "args.px = args.px * args.scale\n",
    "args.py = args.py * args.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading keypoint segmentation model from /home/workspace/src/ctrnet-robot-pose-estimation-ros/weights/panda/panda-3cam_azure/net.pth\n",
      "Camera intrinsics: [[199.8289388    0.         159.9477946 ]\n",
      " [  0.         199.74796549 122.03014119]\n",
      " [  0.           0.           1.        ]]\n",
      "Robot model: Panda\n"
     ]
    }
   ],
   "source": [
    "trans_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_img(cv_img,args):\n",
    "    image_pil = PILImage.fromarray(cv_img)\n",
    "    width, height = image_pil.size\n",
    "    new_size = (int(width*args.scale),int(height*args.scale))\n",
    "    image_pil = image_pil.resize(new_size)\n",
    "    image = trans_to_tensor(image_pil)\n",
    "    return image\n",
    "\n",
    "CtRNet = CtRNet(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8578,  0.5140, -0.0044, -0.1976],\n",
      "         [ 0.0951, -0.1672, -0.9813,  0.3963],\n",
      "         [-0.5051,  0.8414, -0.1923,  1.0495],\n",
      "         [ 0.0000,  0.0000,  0.0000,  1.0000]]], device='cuda:0',\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "cv_img = cv2.imread(os.path.join(args.base_dir,\"images/panda.jpg\"))\n",
    "cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "image = preprocess_img(cv_img,args)\n",
    "if args.use_gpu:\n",
    "    image = image.cuda()\n",
    "\n",
    "joint_angles = np.array([ 0.0200, -0.9641, -0.0662, -2.7979, -0.0469,  1.9289,  0.9137])\n",
    "\n",
    "cTr, points_2d, segmentation, confidence = CtRNet.inference_single_image(image, joint_angles)\n",
    "print(CtRNet.cTr_to_pose_matrix(cTr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.84517401e-16 4.69059663e-16 7.26769741e-16 6.12962313e-16\n",
      " 1.26253929e-16 4.60203752e-16 6.92210954e-16 4.11866715e-16\n",
      " 6.65291398e-16 1.17698333e-16 6.01358349e-16 7.77122305e-16\n",
      " 6.69272347e-16 7.71446680e-16 4.53017547e-16 3.26692255e-16\n",
      " 3.51610695e-16 3.17734422e-16 6.89725784e-16 6.69507308e-16\n",
      " 3.11784262e-16 7.42814434e-16 5.71772411e-16 7.14357854e-16\n",
      " 5.92481020e-16 6.16618019e-16 7.86819527e-16 7.47249766e-16\n",
      " 4.44662661e-16 2.10221347e-16 1.66875885e-16 6.80036148e-16\n",
      " 5.66396030e-16 1.17121658e-16 6.59869116e-18 2.84431907e-16\n",
      " 5.81300955e-16 7.33810419e-16 6.78706467e-16 8.01288419e-16\n",
      " 6.89853247e-16 5.25354770e-16 7.52855316e-16 4.27300643e-16\n",
      " 9.73357063e-19 7.36948431e-16 8.61376340e-17 2.03920212e-16\n",
      " 5.12625441e-16 7.38510449e-16 3.54389373e-16 6.71087340e-16\n",
      " 7.89361212e-16 4.01976521e-16 7.59791123e-16 6.94291568e-16\n",
      " 5.20373357e-16 5.55607144e-16 2.83056500e-16 6.54896205e-16\n",
      " 4.76768218e-16 7.29973978e-16 6.63211922e-16 7.26290274e-16\n",
      " 8.09347841e-17 6.79539979e-16 4.36644640e-16 6.41105436e-16\n",
      " 6.83744633e-16 5.59490833e-16 1.37530775e-17 6.00422955e-16\n",
      " 7.05691281e-16 4.12977302e-17 7.48461638e-16 2.55644935e-16\n",
      " 9.14112784e-17 6.79733908e-16 1.07145798e-16 5.08343990e-16\n",
      " 7.10462760e-16 2.75696465e-16 7.65221603e-16 5.95184821e-16\n",
      " 7.34830794e-16 4.47844828e-16 6.21299533e-16 5.99149003e-16\n",
      " 4.21158434e-16 3.13577026e-16 1.07168909e-16 2.88813199e-16\n",
      " 2.34237657e-16 2.03114477e-16 5.86559678e-16 7.14342936e-16\n",
      " 3.51230133e-16 3.59320488e-16 3.64266081e-16 6.50844153e-16\n",
      " 1.17204419e-16 4.07292754e-16 6.91168094e-16 1.68990903e-16\n",
      " 4.93878522e-16 3.10314536e-17 4.32497203e-16 7.31646317e-16\n",
      " 5.08280992e-16 2.51149691e-16 4.49200687e-16 7.91498789e-16\n",
      " 5.61200470e-16 6.75935125e-16 1.64093603e-16 7.61703974e-16\n",
      " 1.00689150e-16 4.73254104e-16 5.12606289e-16 6.31829894e-16\n",
      " 7.26670484e-17 5.53199167e-16 5.67316401e-16 5.00442274e-16\n",
      " 3.43493679e-16 5.70545208e-16 5.00170661e-16 6.14360987e-16\n",
      " 6.05456635e-16 7.58426647e-16 7.42456097e-16 5.93178885e-16\n",
      " 3.70749549e-16 7.27675735e-16 5.33568712e-16 7.61286325e-16\n",
      " 5.75008286e-18 2.32819417e-16 6.22515264e-16 2.75905503e-16\n",
      " 2.60429159e-16 9.48324726e-19 6.82292910e-16 3.18566217e-16\n",
      " 6.44413071e-16 7.08342828e-16 2.97237195e-16 6.97903449e-16\n",
      " 4.51451652e-16 1.56004793e-17 1.62688422e-16 6.91202220e-16\n",
      " 4.93553357e-16 3.06467339e-16 1.80534433e-16 4.04690581e-16\n",
      " 5.91911516e-16 1.15878223e-16 3.74375308e-16 1.31319918e-16\n",
      " 4.23167088e-16 6.99454264e-16 6.14013188e-16 6.49797755e-16\n",
      " 2.46107094e-16 5.94896375e-16 2.47149498e-16 3.08854400e-16\n",
      " 4.28665825e-16 5.68888250e-18 4.07388711e-16 5.60110266e-16\n",
      " 6.38043699e-16 5.74071528e-16 1.17568537e-16 7.47434267e-16\n",
      " 7.48505408e-16 6.96207666e-16 3.45923468e-16 7.89183292e-16\n",
      " 2.74787221e-17 2.38916859e-16 3.08050549e-16 3.17023010e-16\n",
      " 7.95673686e-16 6.55955875e-16 6.89934683e-16 2.37606081e-16\n",
      " 4.01413740e-16 2.95400308e-16 5.72490662e-16 4.06096945e-17\n",
      " 5.61760288e-17 6.47060132e-16 6.93024722e-16 2.15391920e-16\n",
      " 6.75797635e-16 7.77496657e-16 5.19574225e-16 1.33027143e-16]\n"
     ]
    }
   ],
   "source": [
    "init_std = np.array([\n",
    "                        1.0e-2, 1.0e-2, 1.0e-2, # ori\n",
    "                        1.0e-3, 1.0e-3, 1.0e-3, # pos\n",
    "                    ])\n",
    "pred_std = np.array([1.0e-4, 1.0e-4, 1.0e-4,\n",
    "                     2.5e-5, 2.5e-5, 2.5e-5])\n",
    "pf = ParticleFilter(num_states=6,\n",
    "                    init_distribution=sample_gaussian,\n",
    "                    motion_model=additive_gaussian,\n",
    "                    obs_model=point_feature_obs,\n",
    "                    num_particles=200)\n",
    "pf.init_filter(init_std)\n",
    "\n",
    "pf.predict(pred_std)\n",
    "\n",
    "cam = None\n",
    "gamma = 0.15\n",
    "pf.update(points_2d, CtRNet, joint_angles, cam, cTr, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
