{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "base_dir = \"/home/jingpei/Desktop/CtRNet-robot-pose-estimation\"\n",
    "sys.path.append(base_dir)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as PILImage\n",
    "import cv2\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from models.CtRNet import CtRNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "args.base_dir = \"/home/jingpei/Desktop/CtRNet-robot-pose-estimation\"\n",
    "args.use_gpu = True\n",
    "args.trained_on_multi_gpus = False\n",
    "args.keypoint_seg_model_path = os.path.join(args.base_dir,\"weights/baxter/net.pth\")\n",
    "args.urdf_file = os.path.join(args.base_dir,\"urdfs/Baxter/baxter_description/urdf/baxter.urdf\")\n",
    "\n",
    "args.robot_name = 'Baxter_left_arm' # \"Panda\" or \"Baxter_left_arm\"\n",
    "args.n_kp = 7\n",
    "args.height = 1536\n",
    "args.width = 2048\n",
    "args.fx, args.fy, args.px, args.py = 960.41357421875, 960.22314453125, 1021.7171020507812, 776.2381591796875\n",
    "args.scale = 0.3125  # scale the input image size to (640,480)\n",
    "\n",
    "# scale the camera parameters\n",
    "args.width = int(args.width * args.scale)\n",
    "args.height = int(args.height * args.scale)\n",
    "args.fx = args.fx * args.scale\n",
    "args.fy = args.fy * args.scale\n",
    "args.px = args.px * args.scale\n",
    "args.py = args.py * args.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading keypoint segmentation model from /home/jingpei/Desktop/CtRNet-robot-pose-estimation/weights/baxter/net.pth\n",
      "Camera intrinsics: [[300.12924194   0.         319.28659439]\n",
      " [  0.         300.06973267 242.57442474]\n",
      " [  0.           0.           1.        ]]\n",
      "Robot model: Baxter_left_arm\n"
     ]
    }
   ],
   "source": [
    "trans_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_img(cv_img,args):\n",
    "    image_pil = PILImage.fromarray(cv_img)\n",
    "    width, height = image_pil.size\n",
    "    new_size = (int(width*args.scale),int(height*args.scale))\n",
    "    image_pil = image_pil.resize(new_size)\n",
    "    image = trans_to_tensor(image_pil)\n",
    "    return image\n",
    "\n",
    "CtRNet = CtRNet(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = os.path.join(args.base_dir,\"data/baxter_data/baxter-real-dataset/ground_truth_data\")\n",
    "infile = open(filename,'rb')\n",
    "ground_truth = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "result_dict = dict()\n",
    "count = 0\n",
    "\n",
    "for pose_idx in range(20):\n",
    "    path = os.path.join(args.base_dir,\"data/baxter_data/baxter-real-dataset/pose_\" + str(pose_idx) + \"/*.png\")\n",
    "    file_list = glob.glob(path)\n",
    "\n",
    "    for image_file in file_list:\n",
    "\n",
    "        cv_img = cv2.imread(image_file)\n",
    "        cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n",
    "        image = preprocess_img(cv_img,args)\n",
    "\n",
    "        if args.use_gpu:\n",
    "            image = image.cuda()\n",
    "\n",
    "        joint_angles = np.array(ground_truth[\"pose_\" + str(pose_idx)]['joints'])\n",
    "\n",
    "        cTr, points_2d, segmentation = CtRNet.inference_single_image(image, joint_angles)\n",
    "\n",
    "        _,point_3d = CtRNet.robot.get_joint_RT(np.array(joint_angles))\n",
    "\n",
    "        tmp_dict = dict()\n",
    "        tmp_dict['point_2d'] = points_2d.detach().cpu().numpy().squeeze()\n",
    "        tmp_dict['point_3d'] = point_3d\n",
    "        tmp_dict['cTb'] = cTr.detach().cpu().numpy().squeeze()\n",
    "        result_dict[image_file] = tmp_dict\n",
    "\n",
    "        count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baxter_vis = Baxter_visualization(fx=960.41357421875, fy=960.22314453125, px=1021.7171020507812, py=776.2381591796875, D=None)\n",
    "\n",
    "def get_ee_position(cTb,joints,baxter_vis):\n",
    "\n",
    "    rvec = cTb[:3]\n",
    "    tvec = cTb[3:].reshape(-1,1)\n",
    "    R,_ = cv2.Rodrigues(rvec)\n",
    "    quat_tmp = quaternions.mat2quat(R)\n",
    "    quat = [quat_tmp[1],quat_tmp[2],quat_tmp[3],quat_tmp[0]]\n",
    "\n",
    "    pose = np.hstack((R,tvec))\n",
    "    pred_pose = np.vstack((pose,[0,0,0,1]))\n",
    "    \n",
    "    ee_3d = pred_pose @ baxter_vis.get_bl_T_Jn(8,joints) @ np.array([0,0,0,1]).reshape((-1,1))\n",
    "    ee_3d = baxter_vis.dehomogenize_3d(ee_3d).reshape(-1)\n",
    "    \n",
    "    #ee_2d = get_coor_by_P(ee_3d)\n",
    "    P = baxter_vis.get_camera_matrix()\n",
    "    ee_2d = baxter_vis.dehomogenize_2d(P @ ee_3d.reshape(3,1))\n",
    "    ee_2d = ee_2d.reshape(-1)\n",
    "    \n",
    "    return ee_3d, ee_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_2d_list = []\n",
    "err_3d_list = []\n",
    "for pose_idx in range(20):\n",
    "    path = os.path.join(args.base_dir,\"data/baxter_data/baxter-real-dataset/pose_\" + str(pose_idx) + \"/*.png\")\n",
    "    file_list = glob.glob(path)\n",
    "    gt_2d = np.array(ground_truth[\"pose_\" + str(pose_idx)]['ee_2d'])\n",
    "    gt_3d = np.array(ground_truth[\"pose_\" + str(pose_idx)]['ee_3d'])\n",
    "\n",
    "    for image_file in file_list:\n",
    "        points_2d = result_dict[image_file]['point_2d']\n",
    "        points_3d = result_dict[image_file]['point_3d']\n",
    "        cTb = result_dict[image_file]['cTb']\n",
    "        \n",
    "        joints = ground_truth[\"pose_\" + str(pose_idx)]['joints']\n",
    "        \n",
    "        ee_3d, ee_2d = get_ee_position(cTb,joints,baxter_vis)\n",
    "        err_2d = np.linalg.norm(gt_2d - ee_2d)\n",
    "        err_3d = np.linalg.norm(gt_3d - ee_3d)\n",
    "        err_2d_list.append(err_2d)\n",
    "        err_3d_list.append(err_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.628338259399516"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean 2d error\n",
    "err_2d_list = np.array(err_2d_list)\n",
    "np.mean(err_2d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9394499999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pck_auc\n",
    "pck_2d = []\n",
    "for i in range(200):\n",
    "    num = np.sum(err_2d_list < i)\n",
    "    pck_2d.append(num/100)\n",
    "\n",
    "AUC_2d = np.sum(pck_2d) / len(pck_2d)\n",
    "AUC_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pck @50\n",
    "pck_2d[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06381392332167647"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean 3d error\n",
    "err_3d_list = np.array(err_3d_list)\n",
    "np.mean(err_3d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8393"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add_auc\n",
    "add_3d = []\n",
    "err_3d_list = err_3d_list*1000\n",
    "for i in range(400):\n",
    "    num = np.sum(err_3d_list < i)\n",
    "    add_3d.append(num/100)\n",
    "\n",
    "AUC_3d = np.sum(add_3d) / len(add_3d)\n",
    "AUC_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add @100\n",
    "add_3d[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_ros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
